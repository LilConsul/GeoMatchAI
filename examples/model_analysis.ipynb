{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d169184a699b317",
   "metadata": {},
   "source": [
    "# GeoMatchAI Model Analysis & Visualization\n",
    "\n",
    "This notebook analyzes model performance and creates beautiful visualizations.\n",
    "\n",
    "**Prerequisites:** Run `usertest_comprehensive.py` first to generate test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e2f5fc9ac44e4",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "Run this cell first to install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eff9aa509a56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"adjustText\"]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"✓ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        print(f\"✓ {package} installed\")\n",
    "\n",
    "print(\"\\nAll dependencies ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552901384c6bc216",
   "metadata": {},
   "source": "## 1. Setup & Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14751f40e8d26da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54de637ef66ea810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style settings\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5139f62683738b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "PROJECT_ROOT = Path().absolute().parent\n",
    "OUTPUT_DIR = Path(\"output/csv\")\n",
    "IMG_DIR = PROJECT_ROOT / \"img\"\n",
    "IMG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"Image dir: {IMG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e9677d1ec9c79",
   "metadata": {},
   "source": [
    "## 2. Load Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649fdaf3fed5709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_summary = pd.read_csv(OUTPUT_DIR / \"results_summary.csv\")\n",
    "    df_by_image = pd.read_csv(OUTPUT_DIR / \"results_by_image.csv\")\n",
    "    df_by_model = pd.read_csv(OUTPUT_DIR / \"results_by_model.csv\")\n",
    "    df_by_landmark = pd.read_csv(OUTPUT_DIR / \"results_by_landmark.csv\")\n",
    "\n",
    "    print(\"✓ All CSV files loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"\\nPlease run the comprehensive test first:\")\n",
    "    print(\"  uv run examples/usertest_comprehensive.py\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5b5e50d4d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset sizes:\")\n",
    "print(f\"  - Summary: {len(df_summary):,} rows\")\n",
    "print(f\"  - By Image: {len(df_by_image):,} rows\")\n",
    "print(f\"  - By Model: {len(df_by_model):,} rows\")\n",
    "print(f\"  - By Landmark: {len(df_by_landmark):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8939add64386d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be53bc7bd0a203a",
   "metadata": {},
   "source": [
    "## 3. Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc1931afb82092",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tests = len(df_summary)\n",
    "total_correct = df_summary[\"is_correct\"].sum()\n",
    "overall_accuracy = (total_correct / total_tests) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total tests: {total_tests:,}\")\n",
    "print(f\"Correct predictions: {total_correct:,}\")\n",
    "print(f\"Overall accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae2d53b4fcadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique models tested: {df_summary['model_name'].nunique()}\")\n",
    "print(f\"Unique landmarks: {df_summary['landmark_name'].nunique()}\")\n",
    "print(f\"Unique images: {df_summary['image_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c5a53f77f7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy by preprocessing:\")\n",
    "for prep in [True, False]:\n",
    "    subset = df_summary[df_summary[\"preprocessing\"] == prep]\n",
    "    acc = (subset[\"is_correct\"].sum() / len(subset)) * 100\n",
    "    print(f\"  {'WITH' if prep else 'WITHOUT'} preprocessing: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc6194ffab31f9",
   "metadata": {},
   "source": [
    "## 4. Model Accuracy Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871e0699ccb85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# With preprocessing\n",
    "top_models_with = df_by_model[df_by_model[\"preprocessing\"] == True].nlargest(15, \"accuracy\")\n",
    "ax1.barh(range(len(top_models_with)), top_models_with[\"accuracy\"] * 100)\n",
    "ax1.set_yticks(range(len(top_models_with)))\n",
    "ax1.set_yticklabels(top_models_with[\"model_name\"], fontsize=9)\n",
    "ax1.set_xlabel(\"Accuracy (%)\")\n",
    "ax1.set_title(\"Top 15 Models (WITH Preprocessing)\", fontweight=\"bold\")\n",
    "ax1.grid(axis=\"x\", alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(top_models_with[\"accuracy\"] * 100):\n",
    "    ax1.text(v + 1, i, f\"{v:.1f}%\", va=\"center\", fontsize=8)\n",
    "\n",
    "# Without preprocessing\n",
    "top_models_without = df_by_model[df_by_model[\"preprocessing\"] == False].nlargest(15, \"accuracy\")\n",
    "ax2.barh(range(len(top_models_without)), top_models_without[\"accuracy\"] * 100, color=\"coral\")\n",
    "ax2.set_yticks(range(len(top_models_without)))\n",
    "ax2.set_yticklabels(top_models_without[\"model_name\"], fontsize=9)\n",
    "ax2.set_xlabel(\"Accuracy (%)\")\n",
    "ax2.set_title(\"Top 15 Models (WITHOUT Preprocessing)\", fontweight=\"bold\")\n",
    "ax2.grid(axis=\"x\", alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(top_models_without[\"accuracy\"] * 100):\n",
    "    ax2.text(v + 1, i, f\"{v:.1f}%\", va=\"center\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / \"model_accuracy_rankings.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✓ Saved: {IMG_DIR / 'model_accuracy_rankings.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b291c6c54ffc415",
   "metadata": {},
   "source": [
    "## 5. Discrimination Gap Analysis\n",
    "\n",
    "The **discrimination gap** measures how well a model separates related images from unrelated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd226d74397f9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "df_disc = df_by_model[df_by_model[\"preprocessing\"] == True].nlargest(20, \"discrimination_gap\")\n",
    "\n",
    "y_pos = np.arange(len(df_disc))\n",
    "related_scores = df_disc[\"avg_related_score\"].values * 100\n",
    "unrelated_scores = df_disc[\"avg_unrelated_score\"].values * 100\n",
    "\n",
    "ax.barh(y_pos, related_scores, label=\"Related Images\", alpha=0.8, color=\"green\")\n",
    "ax.barh(y_pos, unrelated_scores, label=\"Unrelated Images\", alpha=0.8, color=\"red\")\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(df_disc[\"model_name\"], fontsize=9)\n",
    "ax.set_xlabel(\"Similarity Score (%)\")\n",
    "ax.set_title(\"Top 20 Models by Discrimination Gap (WITH Preprocessing)\", fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for i, (rel, unrel) in enumerate(zip(related_scores, unrelated_scores)):\n",
    "    gap = rel - unrel\n",
    "    ax.text(max(rel, unrel) + 2, i, f\"Δ{gap:.1f}%\", va=\"center\", fontsize=8, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / \"discrimination_gap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✓ Saved: {IMG_DIR / 'discrimination_gap.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a40c2bb6f43e0d",
   "metadata": {},
   "source": [
    "## 6. Speed vs Accuracy Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fcfe60beb68ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "df_perf = df_by_model[df_by_model[\"preprocessing\"] == True].copy()\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    df_perf[\"avg_inference_time_s\"] * 1000,\n",
    "    df_perf[\"accuracy\"] * 100,\n",
    "    s=200,\n",
    "    alpha=0.7,\n",
    "    c=df_perf[\"discrimination_gap\"],\n",
    "    cmap=\"viridis\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "\n",
    "# Annotate ALL models with adjusted positions to avoid overlap\n",
    "from adjustText import adjust_text\n",
    "\n",
    "texts = []\n",
    "for _, row in df_perf.iterrows():\n",
    "    texts.append(\n",
    "        ax.text(\n",
    "            row[\"avg_inference_time_s\"] * 1000,\n",
    "            row[\"accuracy\"] * 100,\n",
    "            row[\"model_name\"],\n",
    "            fontsize=7,\n",
    "            alpha=0.9,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Try to adjust text positions (if adjustText available), otherwise use basic offset\n",
    "try:\n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle=\"-\", color=\"gray\", alpha=0.5))\n",
    "except:\n",
    "    # Fallback: just offset the labels\n",
    "    for text in texts:\n",
    "        text.set_position((text.get_position()[0] + 2, text.get_position()[1] + 0.5))\n",
    "\n",
    "ax.set_xlabel(\"Inference Time (ms)\")\n",
    "ax.set_ylabel(\"Accuracy (%)\")\n",
    "ax.set_title(\"Speed vs Accuracy Trade-off (WITH Preprocessing)\", fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label(\"Discrimination Gap\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / \"speed_vs_accuracy.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✓ Saved: {IMG_DIR / 'speed_vs_accuracy.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ab9d486a75950",
   "metadata": {},
   "source": [
    "## 7. Landmark Difficulty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd61163d77debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Related landmarks accuracy\n",
    "related_landmarks = df_by_landmark[\n",
    "    (df_by_landmark[\"is_related\"] == True) & (df_by_landmark[\"preprocessing\"] == True)\n",
    "].sort_values(\"accuracy\", ascending=True)\n",
    "\n",
    "if len(related_landmarks) > 0:\n",
    "    ax1.barh(\n",
    "        range(len(related_landmarks)), related_landmarks[\"accuracy\"] * 100, color=\"forestgreen\"\n",
    "    )\n",
    "    ax1.set_yticks(range(len(related_landmarks)))\n",
    "    ax1.set_yticklabels(related_landmarks[\"landmark_name\"])\n",
    "    ax1.set_xlabel(\"Accuracy (%)\")\n",
    "    ax1.set_title(\"Landmark Recognition Accuracy\\n(Related Images)\", fontweight=\"bold\")\n",
    "    ax1.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "    for i, v in enumerate(related_landmarks[\"accuracy\"] * 100):\n",
    "        ax1.text(v + 1, i, f\"{v:.1f}%\", va=\"center\", fontsize=9)\n",
    "\n",
    "# Score distribution by landmark\n",
    "landmark_scores = (\n",
    "    df_summary[(df_summary[\"preprocessing\"] == True) & (df_summary[\"is_related\"] == True)]\n",
    "    .groupby(\"landmark_name\")[\"similarity_score\"]\n",
    "    .apply(list)\n",
    ")\n",
    "\n",
    "if len(landmark_scores) > 0:\n",
    "    bp = ax2.boxplot(landmark_scores.values, vert=False, patch_artist=True)\n",
    "    ax2.set_yticklabels(landmark_scores.index)\n",
    "    ax2.set_yticks(range(1, len(landmark_scores) + 1))\n",
    "    for patch in bp[\"boxes\"]:\n",
    "        patch.set_facecolor(\"lightblue\")\n",
    "        patch.set_alpha(0.7)\n",
    "    ax2.set_xlabel(\"Similarity Score\")\n",
    "    ax2.set_title(\"Score Distribution by Landmark\", fontweight=\"bold\")\n",
    "    ax2.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / \"landmark_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✓ Saved: {IMG_DIR / 'landmark_analysis.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3716a3c2bbf59",
   "metadata": {},
   "source": [
    "## 8. Model Performance Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926ac5de9b34273",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_4_models = (\n",
    "    df_by_model[df_by_model[\"preprocessing\"] == True].nlargest(4, \"accuracy\")[\"model_name\"].values\n",
    ")\n",
    "print(f\"Top 4 models: {list(top_4_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5b6db115c579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(top_4_models):\n",
    "    model_data = df_summary[\n",
    "        (df_summary[\"model_name\"] == model_name)\n",
    "        & (df_summary[\"preprocessing\"] == True)\n",
    "        & (df_summary[\"landmark_name\"] != \"unrelated\")  # Exclude 'unrelated' as a landmark\n",
    "    ]\n",
    "\n",
    "    # Create pivot with landmark as rows, showing avg scores for related vs unrelated\n",
    "    pivot = model_data.pivot_table(\n",
    "        index=\"landmark_name\", columns=\"is_related\", values=\"similarity_score\", aggfunc=\"mean\"\n",
    "    )\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    if pivot.columns.tolist() == [False, True]:\n",
    "        pivot.columns = [\"Unrelated\", \"Related\"]\n",
    "    elif pivot.columns.tolist() == [True]:\n",
    "        pivot.columns = [\"Related\"]\n",
    "\n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"RdYlGn\",\n",
    "        center=0.65,\n",
    "        vmin=0.3,\n",
    "        vmax=0.9,\n",
    "        ax=axes[idx],\n",
    "        cbar_kws={\"label\": \"Score\", \"shrink\": 0.8},\n",
    "        annot_kws={\"fontsize\": 10, \"fontweight\": \"bold\"},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"white\",\n",
    "    )\n",
    "\n",
    "    # Shorten model name if too long\n",
    "    short_name = model_name if len(model_name) < 30 else model_name[:27] + \"...\"\n",
    "    axes[idx].set_title(f\"{short_name}\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"\")\n",
    "    axes[idx].set_ylabel(\"\")\n",
    "    axes[idx].tick_params(axis=\"both\", labelsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / \"model_heatmaps.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✓ Saved: {IMG_DIR / 'model_heatmaps.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4b9c611ae9e04",
   "metadata": {},
   "source": [
    "## 9. Preprocessing Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e65eb84f5f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate preprocessing delta for each model\n",
    "preprocessing_impact = []\n",
    "\n",
    "for model_name in df_by_model[\"model_name\"].unique():\n",
    "    with_prep = df_by_model[\n",
    "        (df_by_model[\"model_name\"] == model_name) & (df_by_model[\"preprocessing\"] == True)\n",
    "    ][\"accuracy\"].values\n",
    "\n",
    "    without_prep = df_by_model[\n",
    "        (df_by_model[\"model_name\"] == model_name) & (df_by_model[\"preprocessing\"] == False)\n",
    "    ][\"accuracy\"].values\n",
    "\n",
    "    if len(with_prep) > 0 and len(without_prep) > 0:\n",
    "        delta = (with_prep[0] - without_prep[0]) * 100\n",
    "        preprocessing_impact.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"delta\": delta,\n",
    "                \"with\": with_prep[0] * 100,\n",
    "                \"without\": without_prep[0] * 100,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_impact = pd.DataFrame(preprocessing_impact).sort_values(\"delta\", ascending=False)\n",
    "df_impact.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ddf7c9341bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Impact by model\n",
    "top_15 = df_impact.head(15)\n",
    "colors = [\"green\" if x > 0 else \"red\" for x in top_15[\"delta\"]]\n",
    "ax1.barh(range(len(top_15)), top_15[\"delta\"], color=colors, alpha=0.7)\n",
    "ax1.set_yticks(range(len(top_15)))\n",
    "ax1.set_yticklabels(top_15[\"model\"], fontsize=9)\n",
    "ax1.set_xlabel(\"Accuracy Change (%)\")\n",
    "ax1.set_title(\"Preprocessing Impact (Positive = Better WITH)\", fontweight=\"bold\")\n",
    "ax1.axvline(x=0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "ax1.grid(axis=\"x\", alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(top_15[\"delta\"]):\n",
    "    ax1.text(\n",
    "        v + 0.2 if v > 0 else v - 0.2,\n",
    "        i,\n",
    "        f\"{v:+.1f}%\",\n",
    "        va=\"center\",\n",
    "        ha=\"left\" if v > 0 else \"right\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "# Distribution comparison\n",
    "with_prep_acc = df_by_model[df_by_model[\"preprocessing\"] == True][\"accuracy\"] * 100\n",
    "without_prep_acc = df_by_model[df_by_model[\"preprocessing\"] == False][\"accuracy\"] * 100\n",
    "\n",
    "ax2.hist(\n",
    "    [with_prep_acc, without_prep_acc],\n",
    "    bins=20,\n",
    "    label=[\"WITH Preprocessing\", \"WITHOUT Preprocessing\"],\n",
    "    alpha=0.7,\n",
    "    color=[\"green\", \"red\"],\n",
    ")\n",
    "ax2.set_xlabel(\"Accuracy (%)\")\n",
    "ax2.set_ylabel(\"Number of Models\")\n",
    "ax2.set_title(\"Accuracy Distribution\", fontweight=\"bold\")\n",
    "ax2.legend()\n",
    "ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / \"preprocessing_impact.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"✓ Saved: {IMG_DIR / 'preprocessing_impact.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3869588b65cf4",
   "metadata": {},
   "source": "## 10. Export Text Report"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7755889f2b84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "lines.append(\"=\" * 80)\n",
    "lines.append(\"GeoMatchAI Model Analysis Report\")\n",
    "lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "lines.append(\"=\" * 80)\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"OVERALL STATISTICS\")\n",
    "lines.append(\"-\" * 80)\n",
    "lines.append(f\"Total tests: {len(df_summary):,}\")\n",
    "lines.append(f\"Overall accuracy: {overall_accuracy:.2f}%\")\n",
    "lines.append(f\"Models tested: {df_summary['model_name'].nunique()}\")\n",
    "lines.append(f\"Landmarks tested: {df_summary['landmark_name'].nunique()}\")\n",
    "lines.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa998265c3122d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.append(\"TOP 10 MODELS (WITH PREPROCESSING)\")\n",
    "lines.append(\"-\" * 80)\n",
    "top_10_w = df_by_model[df_by_model[\"preprocessing\"] == True].nlargest(10, \"accuracy\")\n",
    "for i, (_, row) in enumerate(top_10_w.iterrows(), 1):\n",
    "    lines.append(\n",
    "        f\"{i:2d}. {row['model_name']:<35} Acc: {row['accuracy'] * 100:5.2f}%  Gap: {row['discrimination_gap']:.3f}\"\n",
    "    )\n",
    "lines.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d641adbba0948",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.append(\"TOP 10 MODELS (WITHOUT PREPROCESSING)\")\n",
    "lines.append(\"-\" * 80)\n",
    "top_10_wo = df_by_model[df_by_model[\"preprocessing\"] == False].nlargest(10, \"accuracy\")\n",
    "for i, (_, row) in enumerate(top_10_wo.iterrows(), 1):\n",
    "    lines.append(\n",
    "        f\"{i:2d}. {row['model_name']:<35} Acc: {row['accuracy'] * 100:5.2f}%  Gap: {row['discrimination_gap']:.3f}\"\n",
    "    )\n",
    "lines.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647d7cb3e54c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.append(\"IMAGES GENERATED\")\n",
    "lines.append(\"-\" * 80)\n",
    "for img in [\n",
    "    \"model_accuracy_rankings.png\",\n",
    "    \"discrimination_gap.png\",\n",
    "    \"speed_vs_accuracy.png\",\n",
    "    \"landmark_analysis.png\",\n",
    "    \"model_heatmaps.png\",\n",
    "    \"preprocessing_impact.png\",\n",
    "    \"architecture_comparison.png\",\n",
    "    \"summary_dashboard.png\",\n",
    "]:\n",
    "    lines.append(f\"  ✓ {img}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"=\" * 80)\n",
    "\n",
    "report = \"\\n\".join(lines)\n",
    "(IMG_DIR / \"analysis_report.txt\").write_text(report)\n",
    "print(report)\n",
    "print(f\"\\n✓ Saved: {IMG_DIR / 'analysis_report.txt'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

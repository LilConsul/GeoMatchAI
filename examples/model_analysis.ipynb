{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoMatchAI Model Analysis & Visualization\n",
    "\n",
    "This notebook analyzes model performance and creates beautiful visualizations.\n",
    "\n",
    "**Prerequisites:** Run `usertest_comprehensive.py` first to generate test data."
   ],
   "id": "1d169184a699b317"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "Run this cell first to install required packages."
   ],
   "id": "d68e2f5fc9ac44e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['pandas', 'numpy', 'matplotlib', 'seaborn']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"✓ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
    "        print(f\"✓ {package} installed\")\n",
    "\n",
    "print(\"\\nAll dependencies ready!\")"
   ],
   "id": "84eff9aa509a56d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Setup & Imports",
   "id": "552901384c6bc216"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "14751f40e8d26da7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12"
   ],
   "id": "54de637ef66ea810"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "PROJECT_ROOT = Path().absolute().parent\n",
    "OUTPUT_DIR = Path('output/csv')\n",
    "IMG_DIR = PROJECT_ROOT / 'img'\n",
    "IMG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"Image dir: {IMG_DIR}\")"
   ],
   "id": "f5139f62683738b3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Results"
   ],
   "id": "6b1e9677d1ec9c79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_summary = pd.read_csv(OUTPUT_DIR / 'results_summary.csv')\n",
    "    df_by_image = pd.read_csv(OUTPUT_DIR / 'results_by_image.csv')\n",
    "    df_by_model = pd.read_csv(OUTPUT_DIR / 'results_by_model.csv')\n",
    "    df_by_landmark = pd.read_csv(OUTPUT_DIR / 'results_by_landmark.csv')\n",
    "    \n",
    "    print(\"✓ All CSV files loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"\\nPlease run the comprehensive test first:\")\n",
    "    print(\"  uv run examples/usertest_comprehensive.py\")\n",
    "    raise"
   ],
   "id": "649fdaf3fed5709f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset sizes:\")\n",
    "print(f\"  - Summary: {len(df_summary):,} rows\")\n",
    "print(f\"  - By Image: {len(df_by_image):,} rows\")\n",
    "print(f\"  - By Model: {len(df_by_model):,} rows\")\n",
    "print(f\"  - By Landmark: {len(df_by_landmark):,} rows\")"
   ],
   "id": "64e5b5e50d4d153"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "df_summary.head()"
   ],
   "id": "ad8939add64386d2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overall Statistics"
   ],
   "id": "9be53bc7bd0a203a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tests = len(df_summary)\n",
    "total_correct = df_summary['is_correct'].sum()\n",
    "overall_accuracy = (total_correct / total_tests) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total tests: {total_tests:,}\")\n",
    "print(f\"Correct predictions: {total_correct:,}\")\n",
    "print(f\"Overall accuracy: {overall_accuracy:.2f}%\")"
   ],
   "id": "aecc1931afb82092"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique models tested: {df_summary['model_name'].nunique()}\")\n",
    "print(f\"Unique landmarks: {df_summary['landmark_name'].nunique()}\")\n",
    "print(f\"Unique images: {df_summary['image_name'].nunique()}\")"
   ],
   "id": "c6ae2d53b4fcadb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy by preprocessing:\")\n",
    "for prep in [True, False]:\n",
    "    subset = df_summary[df_summary['preprocessing'] == prep]\n",
    "    acc = (subset['is_correct'].sum() / len(subset)) * 100\n",
    "    print(f\"  {'WITH' if prep else 'WITHOUT'} preprocessing: {acc:.2f}%\")"
   ],
   "id": "728c5a53f77f7a38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Accuracy Rankings"
   ],
   "id": "6ccc6194ffab31f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# With preprocessing\n",
    "top_models_with = df_by_model[df_by_model['preprocessing'] == True].nlargest(15, 'accuracy')\n",
    "ax1.barh(range(len(top_models_with)), top_models_with['accuracy'] * 100)\n",
    "ax1.set_yticks(range(len(top_models_with)))\n",
    "ax1.set_yticklabels(top_models_with['model_name'], fontsize=9)\n",
    "ax1.set_xlabel('Accuracy (%)')\n",
    "ax1.set_title('Top 15 Models (WITH Preprocessing)', fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(top_models_with['accuracy'] * 100):\n",
    "    ax1.text(v + 1, i, f'{v:.1f}%', va='center', fontsize=8)\n",
    "\n",
    "# Without preprocessing\n",
    "top_models_without = df_by_model[df_by_model['preprocessing'] == False].nlargest(15, 'accuracy')\n",
    "ax2.barh(range(len(top_models_without)), top_models_without['accuracy'] * 100, color='coral')\n",
    "ax2.set_yticks(range(len(top_models_without)))\n",
    "ax2.set_yticklabels(top_models_without['model_name'], fontsize=9)\n",
    "ax2.set_xlabel('Accuracy (%)')\n",
    "ax2.set_title('Top 15 Models (WITHOUT Preprocessing)', fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(top_models_without['accuracy'] * 100):\n",
    "    ax2.text(v + 1, i, f'{v:.1f}%', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / 'model_accuracy_rankings.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {IMG_DIR / 'model_accuracy_rankings.png'}\")\n",
    "plt.show()"
   ],
   "id": "5871e0699ccb85f1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discrimination Gap Analysis\n",
    "\n",
    "The **discrimination gap** measures how well a model separates related images from unrelated ones."
   ],
   "id": "4b291c6c54ffc415"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "df_disc = df_by_model[df_by_model['preprocessing'] == True].nlargest(20, 'discrimination_gap')\n",
    "\n",
    "y_pos = np.arange(len(df_disc))\n",
    "related_scores = df_disc['avg_related_score'].values * 100\n",
    "unrelated_scores = df_disc['avg_unrelated_score'].values * 100\n",
    "\n",
    "ax.barh(y_pos, related_scores, label='Related Images', alpha=0.8, color='green')\n",
    "ax.barh(y_pos, unrelated_scores, label='Unrelated Images', alpha=0.8, color='red')\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(df_disc['model_name'], fontsize=9)\n",
    "ax.set_xlabel('Similarity Score (%)')\n",
    "ax.set_title('Top 20 Models by Discrimination Gap (WITH Preprocessing)', fontweight='bold')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for i, (rel, unrel) in enumerate(zip(related_scores, unrelated_scores)):\n",
    "    gap = rel - unrel\n",
    "    ax.text(max(rel, unrel) + 2, i, f'Δ{gap:.1f}%', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / 'discrimination_gap.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {IMG_DIR / 'discrimination_gap.png'}\")\n",
    "plt.show()"
   ],
   "id": "dd226d74397f9f37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Speed vs Accuracy Trade-off"
   ],
   "id": "44a40c2bb6f43e0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "df_perf = df_by_model[df_by_model['preprocessing'] == True]\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    df_perf['avg_inference_time_s'] * 1000,\n",
    "    df_perf['accuracy'] * 100,\n",
    "    s=200,\n",
    "    alpha=0.6,\n",
    "    c=df_perf['discrimination_gap'],\n",
    "    cmap='viridis',\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# Annotate top performers\n",
    "top_acc = df_perf.nlargest(5, 'accuracy')\n",
    "for _, row in top_acc.iterrows():\n",
    "    ax.annotate(\n",
    "        row['model_name'],\n",
    "        (row['avg_inference_time_s'] * 1000, row['accuracy'] * 100),\n",
    "        xytext=(5, 5),\n",
    "        textcoords='offset points',\n",
    "        fontsize=8,\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5)\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Inference Time (ms)')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Speed vs Accuracy Trade-off (WITH Preprocessing)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Discrimination Gap')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / 'speed_vs_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {IMG_DIR / 'speed_vs_accuracy.png'}\")\n",
    "plt.show()"
   ],
   "id": "e2fcfe60beb68ac5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Landmark Difficulty Analysis"
   ],
   "id": "123ab9d486a75950"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Related landmarks accuracy\n",
    "related_landmarks = df_by_landmark[\n",
    "    (df_by_landmark['is_related'] == True) & \n",
    "    (df_by_landmark['preprocessing'] == True)\n",
    "].sort_values('accuracy', ascending=True)\n",
    "\n",
    "if len(related_landmarks) > 0:\n",
    "    ax1.barh(range(len(related_landmarks)), related_landmarks['accuracy'] * 100, color='forestgreen')\n",
    "    ax1.set_yticks(range(len(related_landmarks)))\n",
    "    ax1.set_yticklabels(related_landmarks['landmark_name'])\n",
    "    ax1.set_xlabel('Accuracy (%)')\n",
    "    ax1.set_title('Landmark Recognition Accuracy\\n(Related Images)', fontweight='bold')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(related_landmarks['accuracy'] * 100):\n",
    "        ax1.text(v + 1, i, f'{v:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "# Score distribution by landmark\n",
    "landmark_scores = df_summary[\n",
    "    (df_summary['preprocessing'] == True) & \n",
    "    (df_summary['is_related'] == True)\n",
    "].groupby('landmark_name')['similarity_score'].apply(list)\n",
    "\n",
    "if len(landmark_scores) > 0:\n",
    "    ax2.boxplot(landmark_scores.values, labels=landmark_scores.index, vert=False)\n",
    "    ax2.set_xlabel('Similarity Score')\n",
    "    ax2.set_title('Score Distribution by Landmark', fontweight='bold')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / 'landmark_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {IMG_DIR / 'landmark_analysis.png'}\")\n",
    "plt.show()"
   ],
   "id": "dfd61163d77debf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Performance Heatmaps"
   ],
   "id": "20c3716a3c2bbf59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_4_models = df_by_model[df_by_model['preprocessing'] == True].nlargest(4, 'accuracy')['model_name'].values\n",
    "print(f\"Top 4 models: {list(top_4_models)}\")"
   ],
   "id": "4926ac5de9b34273"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(top_4_models):\n",
    "    model_data = df_summary[\n",
    "        (df_summary['model_name'] == model_name) & \n",
    "        (df_summary['preprocessing'] == True)\n",
    "    ]\n",
    "    \n",
    "    pivot = model_data.pivot_table(\n",
    "        index='landmark_name',\n",
    "        columns='is_related',\n",
    "        values='similarity_score',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='RdYlGn',\n",
    "        center=0.65,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        ax=axes[idx],\n",
    "        cbar_kws={'label': 'Similarity Score'}\n",
    "    )\n",
    "    \n",
    "    axes[idx].set_title(f'{model_name}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Is Related')\n",
    "    axes[idx].set_ylabel('Landmark')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / 'model_heatmaps.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {IMG_DIR / 'model_heatmaps.png'}\")\n",
    "plt.show()"
   ],
   "id": "29c5b6db115c579f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Preprocessing Impact Analysis"
   ],
   "id": "e8e4b9c611ae9e04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate preprocessing delta for each model\n",
    "preprocessing_impact = []\n",
    "\n",
    "for model_name in df_by_model['model_name'].unique():\n",
    "    with_prep = df_by_model[\n",
    "        (df_by_model['model_name'] == model_name) & \n",
    "        (df_by_model['preprocessing'] == True)\n",
    "    ]['accuracy'].values\n",
    "    \n",
    "    without_prep = df_by_model[\n",
    "        (df_by_model['model_name'] == model_name) & \n",
    "        (df_by_model['preprocessing'] == False)\n",
    "    ]['accuracy'].values\n",
    "    \n",
    "    if len(with_prep) > 0 and len(without_prep) > 0:\n",
    "        delta = (with_prep[0] - without_prep[0]) * 100\n",
    "        preprocessing_impact.append({\n",
    "            'model': model_name,\n",
    "            'delta': delta,\n",
    "            'with': with_prep[0] * 100,\n",
    "            'without': without_prep[0] * 100\n",
    "        })\n",
    "\n",
    "df_impact = pd.DataFrame(preprocessing_impact).sort_values('delta', ascending=False)\n",
    "df_impact.head(10)"
   ],
   "id": "686e65eb84f5f9df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Impact by model\n",
    "top_15 = df_impact.head(15)\n",
    "colors = ['green' if x > 0 else 'red' for x in top_15['delta']]\n",
    "ax1.barh(range(len(top_15)), top_15['delta'], color=colors, alpha=0.7)\n",
    "ax1.set_yticks(range(len(top_15)))\n",
    "ax1.set_yticklabels(top_15['model'], fontsize=9)\n",
    "ax1.set_xlabel('Accuracy Change (%)')\n",
    "ax1.set_title('Preprocessing Impact (Positive = Better WITH)', fontweight='bold')\n",
    "ax1.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(top_15['delta']):\n",
    "    ax1.text(v + 0.2 if v > 0 else v - 0.2, i, f'{v:+.1f}%', \n",
    "             va='center', ha='left' if v > 0 else 'right', fontsize=8)\n",
    "\n",
    "# Distribution comparison\n",
    "with_prep_acc = df_by_model[df_by_model['preprocessing'] == True]['accuracy'] * 100\n",
    "without_prep_acc = df_by_model[df_by_model['preprocessing'] == False]['accuracy'] * 100\n",
    "\n",
    "ax2.hist([with_prep_acc, without_prep_acc], bins=20, \n",
    "         label=['WITH Preprocessing', 'WITHOUT Preprocessing'], \n",
    "         alpha=0.7, color=['green', 'red'])\n",
    "ax2.set_xlabel('Accuracy (%)')\n",
    "ax2.set_ylabel('Number of Models')\n",
    "ax2.set_title('Accuracy Distribution', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / 'preprocessing_impact.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {IMG_DIR / 'preprocessing_impact.png'}\")\n",
    "plt.show()"
   ],
   "id": "f88ddf7c9341bd50"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Architecture Family Comparison"
   ],
   "id": "232c2e63817d4c3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_model(model_name):\n",
    "    \"\"\"Categorize model by architecture family.\"\"\"\n",
    "    name_lower = model_name.lower()\n",
    "    if 'vit' in name_lower or 'deit' in name_lower:\n",
    "        return 'Vision Transformers'\n",
    "    elif 'swin' in name_lower:\n",
    "        return 'Swin Transformers'\n",
    "    elif 'clip' in name_lower:\n",
    "        return 'CLIP Models'\n",
    "    elif 'efficientnet' in name_lower:\n",
    "        return 'EfficientNet'\n",
    "    elif 'convnext' in name_lower:\n",
    "        return 'ConvNeXt'\n",
    "    elif 'resnet' in name_lower or 'resnest' in name_lower:\n",
    "        return 'ResNet Family'\n",
    "    elif 'densenet' in name_lower:\n",
    "        return 'DenseNet'\n",
    "    elif 'regnet' in name_lower:\n",
    "        return 'RegNet'\n",
    "    elif 'nfnet' in name_lower:\n",
    "        return 'NFNet'\n",
    "    elif 'mobile' in name_lower:\n",
    "        return 'MobileNet'\n",
    "    elif 'inception' in name_lower:\n",
    "        return 'Inception'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df_by_model['architecture'] = df_by_model['model_name'].apply(categorize_model)\n",
    "df_by_model['architecture'].value_counts()"
   ],
   "id": "7530ece01c943fbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Stats by architecture\n",
    "arch_stats = df_by_model[df_by_model['preprocessing'] == True].groupby('architecture').agg({\n",
    "    'accuracy': ['mean', 'std'],\n",
    "    'model_name': 'count'\n",
    "}).sort_values(('accuracy', 'mean'), ascending=False)\n",
    "\n",
    "arch_names = arch_stats.index\n",
    "arch_means = arch_stats[('accuracy', 'mean')].values * 100\n",
    "arch_stds = arch_stats[('accuracy', 'std')].values * 100\n",
    "arch_counts = arch_stats[('model_name', 'count')].values\n",
    "\n",
    "y_pos = np.arange(len(arch_names))\n",
    "ax1.barh(y_pos, arch_means, xerr=arch_stds, alpha=0.7, capsize=5)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels([f\"{name} (n={count})\" for name, count in zip(arch_names, arch_counts)])\n",
    "ax1.set_xlabel('Mean Accuracy (%) ± Std')\n",
    "ax1.set_title('Architecture Family Comparison', fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(arch_means):\n",
    "    ax1.text(v + 2, i, f'{v:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "# Box plot\n",
    "arch_data = []\n",
    "arch_labels = []\n",
    "for arch in arch_names:\n",
    "    data = df_by_model[\n",
    "        (df_by_model['architecture'] == arch) & \n",
    "        (df_by_model['preprocessing'] == True)\n",
    "    ]['accuracy'].values * 100\n",
    "    if len(data) > 0:\n",
    "        arch_data.append(data)\n",
    "        arch_labels.append(arch)\n",
    "\n",
    "bp = ax2.boxplot(arch_data, labels=arch_labels, vert=False, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax2.set_xlabel('Accuracy (%)')\n",
    "ax2.set_title('Performance Distribution', fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMG_DIR / 'architecture_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {IMG_DIR / 'architecture_comparison.png'}\")\n",
    "plt.show()"
   ],
   "id": "2ca34f4ad5e53e79"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Dashboard"
   ],
   "id": "519bbaa44164ad6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)"
   ],
   "id": "5806d53c05b0380d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Overall accuracy\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "categories = ['Overall', 'WITH Prep', 'WITHOUT Prep']\n",
    "accuracies = [\n",
    "    overall_accuracy,\n",
    "    (df_summary[df_summary['preprocessing'] == True]['is_correct'].sum() / \n",
    "     len(df_summary[df_summary['preprocessing'] == True])) * 100,\n",
    "    (df_summary[df_summary['preprocessing'] == False]['is_correct'].sum() / \n",
    "     len(df_summary[df_summary['preprocessing'] == False])) * 100\n",
    "]\n",
    "ax1.bar(categories, accuracies, color=['blue', 'green', 'red'], alpha=0.7)\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('Overall Performance', fontweight='bold')\n",
    "ax1.set_ylim([0, 100])\n",
    "for i, v in enumerate(accuracies):\n",
    "    ax1.text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Best model stats\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "best_model = df_by_model[df_by_model['preprocessing'] == True].nlargest(1, 'accuracy').iloc[0]\n",
    "stats_text = f\"\"\"Best Model:\n",
    "{best_model['model_name']}\n",
    "\n",
    "Accuracy: {best_model['accuracy']*100:.2f}%\n",
    "Discrimination: {best_model['discrimination_gap']:.3f}\n",
    "Avg Related: {best_model['avg_related_score']:.3f}\n",
    "Avg Unrelated: {best_model['avg_unrelated_score']:.3f}\n",
    "Inference: {best_model['avg_inference_time_s']*1000:.1f}ms\"\"\"\n",
    "ax2.text(0.1, 0.5, stats_text, fontsize=11, verticalalignment='center', \n",
    "         family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Champion Model', fontweight='bold')\n",
    "\n",
    "# 3. Test distribution\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "test_dist = df_summary['landmark_name'].value_counts()\n",
    "ax3.pie(test_dist.values, labels=test_dist.index, autopct='%1.1f%%', startangle=90)\n",
    "ax3.set_title('Tests by Landmark', fontweight='bold')\n",
    "\n",
    "# 4. Top 10 models\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "top_10 = df_by_model[df_by_model['preprocessing'] == True].nlargest(10, 'accuracy')\n",
    "x = np.arange(len(top_10))\n",
    "width = 0.35\n",
    "ax4.bar(x - width/2, top_10['accuracy'] * 100, width, label='Accuracy', alpha=0.8)\n",
    "ax4.bar(x + width/2, top_10['discrimination_gap'] * 100, width, label='Discrim. Gap', alpha=0.8)\n",
    "ax4.set_ylabel('Percentage')\n",
    "ax4.set_title('Top 10 Models: Accuracy vs Discrimination Gap', fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(top_10['model_name'], rotation=45, ha='right', fontsize=9)\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Inference time\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "times = df_by_model[df_by_model['preprocessing'] == True]['avg_inference_time_s'] * 1000\n",
    "ax5.hist(times, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "ax5.axvline(times.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {times.mean():.1f}ms')\n",
    "ax5.set_xlabel('Inference Time (ms)')\n",
    "ax5.set_ylabel('Count')\n",
    "ax5.set_title('Inference Time Distribution', fontweight='bold')\n",
    "ax5.legend()\n",
    "\n",
    "# 6. Score distribution\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "related = df_summary[(df_summary['is_related'] == True) & (df_summary['preprocessing'] == True)]['similarity_score']\n",
    "unrelated = df_summary[(df_summary['is_related'] == False) & (df_summary['preprocessing'] == True)]['similarity_score']\n",
    "ax6.hist([related, unrelated], bins=30, alpha=0.7, label=['Related', 'Unrelated'], color=['green', 'red'])\n",
    "ax6.axvline(0.65, color='black', linestyle='--', linewidth=2, label='Threshold (0.65)')\n",
    "ax6.set_xlabel('Similarity Score')\n",
    "ax6.set_ylabel('Count')\n",
    "ax6.set_title('Score Distribution', fontweight='bold')\n",
    "ax6.legend()\n",
    "\n",
    "# 7. Model types\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "arch_counts = df_by_model[df_by_model['preprocessing'] == True]['architecture'].value_counts()\n",
    "ax7.pie(arch_counts.values, labels=arch_counts.index, autopct='%1.0f%%', startangle=90)\n",
    "ax7.set_title('Models by Architecture', fontweight='bold')\n",
    "\n",
    "fig.suptitle('GeoMatchAI Comprehensive Test Results', fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig(IMG_DIR / 'summary_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {IMG_DIR / 'summary_dashboard.png'}\")\n",
    "plt.show()"
   ],
   "id": "8ceaf0f8b03ce04c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Text Report"
   ],
   "id": "9be3869588b65cf4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "lines.append(\"=\" * 80)\n",
    "lines.append(\"GeoMatchAI Model Analysis Report\")\n",
    "lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "lines.append(\"=\" * 80)\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"OVERALL STATISTICS\")\n",
    "lines.append(\"-\" * 80)\n",
    "lines.append(f\"Total tests: {len(df_summary):,}\")\n",
    "lines.append(f\"Overall accuracy: {overall_accuracy:.2f}%\")\n",
    "lines.append(f\"Models tested: {df_summary['model_name'].nunique()}\")\n",
    "lines.append(f\"Landmarks tested: {df_summary['landmark_name'].nunique()}\")\n",
    "lines.append(\"\")"
   ],
   "id": "f7755889f2b84304"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.append(\"TOP 10 MODELS (WITH PREPROCESSING)\")\n",
    "lines.append(\"-\" * 80)\n",
    "top_10_w = df_by_model[df_by_model['preprocessing'] == True].nlargest(10, 'accuracy')\n",
    "for i, (_, row) in enumerate(top_10_w.iterrows(), 1):\n",
    "    lines.append(f\"{i:2d}. {row['model_name']:<35} Acc: {row['accuracy']*100:5.2f}%  Gap: {row['discrimination_gap']:.3f}\")\n",
    "lines.append(\"\")"
   ],
   "id": "aaa998265c3122d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.append(\"TOP 10 MODELS (WITHOUT PREPROCESSING)\")\n",
    "lines.append(\"-\" * 80)\n",
    "top_10_wo = df_by_model[df_by_model['preprocessing'] == False].nlargest(10, 'accuracy')\n",
    "for i, (_, row) in enumerate(top_10_wo.iterrows(), 1):\n",
    "    lines.append(f\"{i:2d}. {row['model_name']:<35} Acc: {row['accuracy']*100:5.2f}%  Gap: {row['discrimination_gap']:.3f}\")\n",
    "lines.append(\"\")"
   ],
   "id": "301d641adbba0948"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.append(\"IMAGES GENERATED\")\n",
    "lines.append(\"-\" * 80)\n",
    "for img in ['model_accuracy_rankings.png', 'discrimination_gap.png', 'speed_vs_accuracy.png',\n",
    "            'landmark_analysis.png', 'model_heatmaps.png', 'preprocessing_impact.png',\n",
    "            'architecture_comparison.png', 'summary_dashboard.png']:\n",
    "    lines.append(f\"  ✓ {img}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"=\" * 80)\n",
    "\n",
    "report = \"\\n\".join(lines)\n",
    "(IMG_DIR / 'analysis_report.txt').write_text(report)\n",
    "print(report)\n",
    "print(f\"\\n✓ Saved: {IMG_DIR / 'analysis_report.txt'}\")"
   ],
   "id": "5647d7cb3e54c6c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
